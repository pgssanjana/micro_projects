{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pgssanjana/micro_projects/blob/main/face_recognition_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92,
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "kVaGNeACGsPx",
        "outputId": "d6541c1d-b195-40ae-c448-ee01bc75a553"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e59aa56f-6d45-4e03-acb3-d4ee65c56fea\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e59aa56f-6d45-4e03-acb3-d4ee65c56fea\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (2).json\n",
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n"
          ]
        }
      ],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "# ! kaggle datasets list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lM81wvDwGu66",
        "outputId": "3cc0e8ea-b0b3-4e99-8fcb-234a2592e785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "face-recognition-30.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d pramod722445/face-recognition-30\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3LxTxk-uGu9O"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "zf = \"/content/face-recognition-30.zip\"\n",
        "target_dir = \"/content/input/face-recognition-30/dataset\"\n",
        "zfile = zipfile.ZipFile(zf)\n",
        "zfile.extractall(target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "g_NnFsj7Gu_0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "\n",
        "# style your matplotlib\n",
        "mpl.style.use(\"seaborn-darkgrid\")\n",
        "# run this block\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qCp2b82hGvCS"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4VEWHlJGvEj",
        "outputId": "855fdbd8-4e1e-47d2-d3af-1cbb15ec99c0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Johnny_Galeck',\n",
              " 'Courteney_Cox',\n",
              " 'Jennifer_Aniston',\n",
              " 'aishwarya_rai',\n",
              " 'hardik_pandya',\n",
              " 'manoj_bajpayee',\n",
              " 'Jim_Parsons',\n",
              " 'angelina_jolie',\n",
              " 'David_Schwimmer',\n",
              " 'random_person',\n",
              " 'scarlett_johansson',\n",
              " 'sylvester_stallone',\n",
              " 'mohamed_ali',\n",
              " 'Lisa_Kudrow',\n",
              " 'ronaldo',\n",
              " 'Simon_Helberg',\n",
              " 'Matthew_Perry',\n",
              " 'Pankaj_Tripathi',\n",
              " 'pewdiepie',\n",
              " 'ROHIT_SHARMA',\n",
              " 'dhoni',\n",
              " 'Sachin_Tendulka',\n",
              " 'suresh_raina',\n",
              " 'brad_pitt',\n",
              " 'bhuvan_bam',\n",
              " 'messi',\n",
              " 'virat_kohli',\n",
              " 'arnold_schwarzenegger',\n",
              " 'Kunal_Nayya',\n",
              " 'Matt_LeBlanc']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "files=os.listdir(\"/content/input/face-recognition-30/dataset/dataset/\")\n",
        "files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_2_name={}\n",
        "name_2_index={}"
      ],
      "metadata": {
        "id": "vM1KeJzHawcw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p--jbHzsawki"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9MAXZMDGvHH",
        "outputId": "ce46536f-023e-4c32-a37c-54934b118de5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 508/508 [00:01<00:00, 316.39it/s]\n",
            "100%|██████████| 702/702 [00:02<00:00, 305.64it/s]\n",
            "100%|██████████| 639/639 [00:02<00:00, 305.40it/s]\n",
            "100%|██████████| 711/711 [00:01<00:00, 368.31it/s]\n",
            "100%|██████████| 292/292 [00:00<00:00, 889.77it/s]\n",
            "100%|██████████| 457/457 [00:00<00:00, 903.74it/s]\n",
            "100%|██████████| 639/639 [00:01<00:00, 497.62it/s]\n",
            "100%|██████████| 465/465 [00:01<00:00, 238.49it/s]\n",
            "100%|██████████| 537/537 [00:01<00:00, 324.28it/s]\n",
            "100%|██████████| 2250/2250 [00:02<00:00, 911.10it/s]\n",
            "100%|██████████| 507/507 [00:02<00:00, 233.06it/s]\n",
            "100%|██████████| 480/480 [00:01<00:00, 416.05it/s]\n",
            "100%|██████████| 338/338 [00:00<00:00, 440.21it/s]\n",
            "100%|██████████| 640/640 [00:02<00:00, 308.04it/s]\n",
            "100%|██████████| 418/418 [00:00<00:00, 545.14it/s]\n",
            "100%|██████████| 484/484 [00:01<00:00, 445.44it/s]\n",
            "100%|██████████| 530/530 [00:01<00:00, 438.37it/s]\n",
            "100%|██████████| 354/354 [00:00<00:00, 694.14it/s]\n",
            "100%|██████████| 395/395 [00:00<00:00, 531.78it/s]\n",
            "100%|██████████| 265/265 [00:00<00:00, 1093.43it/s]\n",
            "100%|██████████| 314/314 [00:00<00:00, 983.18it/s]\n",
            "100%|██████████| 354/354 [00:00<00:00, 535.59it/s]\n",
            "100%|██████████| 319/319 [00:00<00:00, 1311.26it/s]\n",
            "100%|██████████| 552/552 [00:01<00:00, 301.37it/s]\n",
            "100%|██████████| 382/382 [00:00<00:00, 860.87it/s]\n",
            "100%|██████████| 432/432 [00:00<00:00, 676.63it/s]\n",
            "100%|██████████| 391/391 [00:00<00:00, 514.17it/s]\n",
            "100%|██████████| 553/553 [00:01<00:00, 420.21it/s]\n",
            "100%|██████████| 532/532 [00:00<00:00, 656.13it/s]\n",
            "100%|██████████| 449/449 [00:01<00:00, 373.66it/s]\n"
          ]
        }
      ],
      "source": [
        "image_array=[]  # it's a list later i will convert it to array\n",
        "label_array=[]\n",
        "path=\"/content/input/face-recognition-30/dataset/dataset/\"\n",
        "# loop through each sub-folder in train\n",
        "for i in range(len(files)):\n",
        "    # files in sub-folder\n",
        "    file_sub=os.listdir(path+files[i])\n",
        "    index_2_name[i]=files[i]\n",
        "    name_2_index[files[i]]=i\n",
        "\n",
        "    for k in tqdm(range(len(file_sub))):\n",
        "        try:\n",
        "            img=cv2.imread(path+files[i]+\"/\"+file_sub[k])\n",
        "            img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
        "            img=cv2.resize(img,(96,96))\n",
        "            image_array.append(img)\n",
        "            label_array.append(i)\n",
        "        except:\n",
        "            pass"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(index_2_name)\n",
        "print()\n",
        "print(name_2_index)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2jJ7uJtPa9oG",
        "outputId": "662cd2c9-620b-452b-9e3a-003f5b0e34a3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 'Johnny_Galeck', 1: 'Courteney_Cox', 2: 'Jennifer_Aniston', 3: 'aishwarya_rai', 4: 'hardik_pandya', 5: 'manoj_bajpayee', 6: 'Jim_Parsons', 7: 'angelina_jolie', 8: 'David_Schwimmer', 9: 'random_person', 10: 'scarlett_johansson', 11: 'sylvester_stallone', 12: 'mohamed_ali', 13: 'Lisa_Kudrow', 14: 'ronaldo', 15: 'Simon_Helberg', 16: 'Matthew_Perry', 17: 'Pankaj_Tripathi', 18: 'pewdiepie', 19: 'ROHIT_SHARMA', 20: 'dhoni', 21: 'Sachin_Tendulka', 22: 'suresh_raina', 23: 'brad_pitt', 24: 'bhuvan_bam', 25: 'messi', 26: 'virat_kohli', 27: 'arnold_schwarzenegger', 28: 'Kunal_Nayya', 29: 'Matt_LeBlanc'}\n",
            "\n",
            "{'Johnny_Galeck': 0, 'Courteney_Cox': 1, 'Jennifer_Aniston': 2, 'aishwarya_rai': 3, 'hardik_pandya': 4, 'manoj_bajpayee': 5, 'Jim_Parsons': 6, 'angelina_jolie': 7, 'David_Schwimmer': 8, 'random_person': 9, 'scarlett_johansson': 10, 'sylvester_stallone': 11, 'mohamed_ali': 12, 'Lisa_Kudrow': 13, 'ronaldo': 14, 'Simon_Helberg': 15, 'Matthew_Perry': 16, 'Pankaj_Tripathi': 17, 'pewdiepie': 18, 'ROHIT_SHARMA': 19, 'dhoni': 20, 'Sachin_Tendulka': 21, 'suresh_raina': 22, 'brad_pitt': 23, 'bhuvan_bam': 24, 'messi': 25, 'virat_kohli': 26, 'arnold_schwarzenegger': 27, 'Kunal_Nayya': 28, 'Matt_LeBlanc': 29}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLa637mRJD4N",
        "outputId": "dd329b5f-1113-4e39-dba9-3e3ae4ade93b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "ejeSc5xLJD1o"
      },
      "outputs": [],
      "source": [
        "image_array=np.array(image_array)/255.0\n",
        "label_array=np.array(label_array)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "KUi9FBYvJDzS"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,Y_train,Y_test=train_test_split(image_array,label_array,test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "hngjBIXiJDwb"
      },
      "outputs": [],
      "source": [
        "from keras import layers,callbacks,utils,applications,optimizers\n",
        "from keras.models import Sequential,Model,load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2SQECTuGvJw",
        "outputId": "df0048c0-a731-4d24-c497-3188026366de"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "len(files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hByioZFKJ7hW"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "# from keras.applications import MobileNet\n",
        "from keras.applications.mobilenet import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "SBZefzqZMA2l"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DhDgzmyAMtTm"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxy3PBeGJm8K",
        "outputId": "8a0f66d3-3e78-49ff-e609-4713a0c7c318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " mobilenetv2_1.00_96 (Functi  (None, 3, 3, 1280)       2257984   \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1280)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 1280)              0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 1281      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,259,265\n",
            "Trainable params: 2,225,153\n",
            "Non-trainable params: 34,112\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "\n",
        "model=Sequential()\n",
        "# I will use MobileNetV2 as an pretrained model \n",
        "pretrained_model=MobileNetV2(input_shape=(96,96,3),include_top=False,\n",
        "                                         weights=\"imagenet\")\n",
        "model.add(pretrained_model)\n",
        "model.add(layers.GlobalAveragePooling2D())\n",
        "# add dropout to increase accuracy by not overfitting\n",
        "model.add(layers.Dropout(0.3))\n",
        "# add dense layer as final output\n",
        "model.add(layers.Dense(1))\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JBioWjzNLIgf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "xUuxnhXBJm5l"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=\"adam\",loss=\"mean_squared_error\",metrics=[\"mae\"])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "UhaGKyvNJm29"
      },
      "outputs": [],
      "source": [
        "\n",
        "ckp_path=\"trained_model/model\"\n",
        "model_checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=ckp_path,\n",
        "                                                   monitor=\"val_mae\",\n",
        "                                                   mode=\"auto\",\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "L6H8N-nhJm0W"
      },
      "outputs": [],
      "source": [
        "# create a lr reducer which decrease learning rate when accuarcy does not increase\n",
        "reduce_lr=tf.keras.callbacks.ReduceLROnPlateau(factor=0.9,monitor=\"val_mae\",\n",
        "                                             mode=\"auto\",cooldown=0,\n",
        "                                             patience=5,verbose=1,min_lr=1e-6)\n",
        "# patience : wait till 5 epoch\n",
        "# verbose : show accuracy every 1 epoch\n",
        "# min_lr=minimum learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "nmqxqiWKPIP_"
      },
      "outputs": [],
      "source": [
        "# import tensorflow as tf\n",
        "# my_callbacks = [\n",
        "#     tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "#     tf.keras.callbacks.ModelCheckpoint(filepath='model.{epoch:02d}-{val_loss:.2f}.h5',\n",
        "#                               monitor='val_loss',\n",
        "#                              verbose=1, \n",
        "#                              save_best_only=True),\n",
        "#     tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TPC7V3cND0P",
        "outputId": "7c85ca68-28b0-40b9-9731-f6ff4d83848d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "212/212 [==============================] - 35s 94ms/step - loss: 35.5083 - mae: 4.2867 - val_loss: 353.4930 - val_mae: 17.5709 - lr: 0.0010\n",
            "Epoch 2/300\n",
            "212/212 [==============================] - 17s 80ms/step - loss: 14.4682 - mae: 2.6398 - val_loss: 277.3805 - val_mae: 15.5134 - lr: 0.0010\n",
            "Epoch 3/300\n",
            "212/212 [==============================] - 17s 82ms/step - loss: 9.9027 - mae: 2.0931 - val_loss: 244.5457 - val_mae: 14.0846 - lr: 0.0010\n",
            "Epoch 4/300\n",
            "212/212 [==============================] - 17s 81ms/step - loss: 6.7342 - mae: 1.6970 - val_loss: 261.1548 - val_mae: 14.4133 - lr: 0.0010\n",
            "Epoch 5/300\n",
            "212/212 [==============================] - 17s 82ms/step - loss: 8.3989 - mae: 1.8974 - val_loss: 295.8155 - val_mae: 15.7423 - lr: 0.0010\n",
            "Epoch 6/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 7.7216 - mae: 1.7868 - val_loss: 323.1360 - val_mae: 16.6243 - lr: 0.0010\n",
            "Epoch 7/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 5.6005 - mae: 1.4749 - val_loss: 180.3157 - val_mae: 11.2781 - lr: 0.0010\n",
            "Epoch 8/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 4.8511 - mae: 1.3867 - val_loss: 134.7267 - val_mae: 10.0170 - lr: 0.0010\n",
            "Epoch 9/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 5.3401 - mae: 1.4090 - val_loss: 65.0211 - val_mae: 6.2940 - lr: 0.0010\n",
            "Epoch 10/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 4.7948 - mae: 1.3091 - val_loss: 55.5357 - val_mae: 5.5905 - lr: 0.0010\n",
            "Epoch 11/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 4.0502 - mae: 1.2574 - val_loss: 45.3852 - val_mae: 5.4154 - lr: 0.0010\n",
            "Epoch 12/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 3.3875 - mae: 1.1299 - val_loss: 28.5381 - val_mae: 3.8231 - lr: 0.0010\n",
            "Epoch 13/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 4.1281 - mae: 1.2187 - val_loss: 35.5993 - val_mae: 4.2853 - lr: 0.0010\n",
            "Epoch 14/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 3.8620 - mae: 1.1621 - val_loss: 57.0414 - val_mae: 5.7563 - lr: 0.0010\n",
            "Epoch 15/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 4.5761 - mae: 1.2642 - val_loss: 33.4931 - val_mae: 4.2252 - lr: 0.0010\n",
            "Epoch 16/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 5.5416 - mae: 1.3685 - val_loss: 51.1837 - val_mae: 5.5616 - lr: 0.0010\n",
            "Epoch 17/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 3.4357 - mae: 1.0937\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0009000000427477062.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 3.4365 - mae: 1.0939 - val_loss: 37.8748 - val_mae: 4.5617 - lr: 0.0010\n",
            "Epoch 18/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 2.4199 - mae: 0.9259 - val_loss: 22.0521 - val_mae: 3.1321 - lr: 9.0000e-04\n",
            "Epoch 19/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 2.8075 - mae: 1.0106 - val_loss: 25.7015 - val_mae: 3.3104 - lr: 9.0000e-04\n",
            "Epoch 20/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 1.6755 - mae: 0.7965 - val_loss: 17.9616 - val_mae: 2.6098 - lr: 9.0000e-04\n",
            "Epoch 21/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 1.3127 - mae: 0.7209 - val_loss: 18.5827 - val_mae: 2.3675 - lr: 9.0000e-04\n",
            "Epoch 22/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 5.9004 - mae: 1.4328 - val_loss: 31.5766 - val_mae: 4.1601 - lr: 9.0000e-04\n",
            "Epoch 23/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 3.5136 - mae: 1.1415 - val_loss: 40.7751 - val_mae: 4.7218 - lr: 9.0000e-04\n",
            "Epoch 24/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 2.2758 - mae: 0.8962 - val_loss: 24.8869 - val_mae: 3.3899 - lr: 9.0000e-04\n",
            "Epoch 25/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 2.2678 - mae: 0.9201 - val_loss: 22.9404 - val_mae: 3.1353 - lr: 9.0000e-04\n",
            "Epoch 26/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 3.6923 - mae: 1.1432\n",
            "Epoch 26: ReduceLROnPlateau reducing learning rate to 0.0008100000384729356.\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 3.6980 - mae: 1.1437 - val_loss: 35.9261 - val_mae: 4.1773 - lr: 9.0000e-04\n",
            "Epoch 27/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 2.0282 - mae: 0.8956 - val_loss: 30.4840 - val_mae: 4.1112 - lr: 8.1000e-04\n",
            "Epoch 28/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.6125 - mae: 0.7987 - val_loss: 24.2196 - val_mae: 3.5785 - lr: 8.1000e-04\n",
            "Epoch 29/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.2269 - mae: 0.6999 - val_loss: 19.7968 - val_mae: 2.8293 - lr: 8.1000e-04\n",
            "Epoch 30/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 5.2478 - mae: 1.3621 - val_loss: 34.4533 - val_mae: 3.9539 - lr: 8.1000e-04\n",
            "Epoch 31/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 2.4908 - mae: 0.9431\n",
            "Epoch 31: ReduceLROnPlateau reducing learning rate to 0.0007290000503417104.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 2.4928 - mae: 0.9435 - val_loss: 16.8501 - val_mae: 2.5900 - lr: 8.1000e-04\n",
            "Epoch 32/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 1.4982 - mae: 0.7732 - val_loss: 15.9971 - val_mae: 2.4376 - lr: 7.2900e-04\n",
            "Epoch 33/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 1.0501 - mae: 0.6452 - val_loss: 13.7719 - val_mae: 1.9809 - lr: 7.2900e-04\n",
            "Epoch 34/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 1.0305 - mae: 0.6581 - val_loss: 11.6215 - val_mae: 2.0155 - lr: 7.2900e-04\n",
            "Epoch 35/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 6.2468 - mae: 1.4315 - val_loss: 22.1993 - val_mae: 3.1875 - lr: 7.2900e-04\n",
            "Epoch 36/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 2.2583 - mae: 0.9013 - val_loss: 42.3087 - val_mae: 4.5718 - lr: 7.2900e-04\n",
            "Epoch 37/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 4.7636 - mae: 1.2669 - val_loss: 27.4552 - val_mae: 3.5122 - lr: 7.2900e-04\n",
            "Epoch 38/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 1.6067 - mae: 0.7650\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 0.0006561000715009868.\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 1.6066 - mae: 0.7649 - val_loss: 13.6350 - val_mae: 2.1021 - lr: 7.2900e-04\n",
            "Epoch 39/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 1.1645 - mae: 0.6718 - val_loss: 11.5009 - val_mae: 1.7506 - lr: 6.5610e-04\n",
            "Epoch 40/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.8886 - mae: 0.6109 - val_loss: 11.9047 - val_mae: 1.8405 - lr: 6.5610e-04\n",
            "Epoch 41/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.9416 - mae: 0.6352 - val_loss: 13.8993 - val_mae: 1.9164 - lr: 6.5610e-04\n",
            "Epoch 42/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 3.2873 - mae: 1.0224 - val_loss: 15.7413 - val_mae: 2.0195 - lr: 6.5610e-04\n",
            "Epoch 43/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 1.3042 - mae: 0.7051 - val_loss: 11.4140 - val_mae: 1.5770 - lr: 6.5610e-04\n",
            "Epoch 44/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.8977 - mae: 0.6114 - val_loss: 9.5683 - val_mae: 1.4700 - lr: 6.5610e-04\n",
            "Epoch 45/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 1.4474 - mae: 0.7487 - val_loss: 11.4715 - val_mae: 1.6661 - lr: 6.5610e-04\n",
            "Epoch 46/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 1.3453 - mae: 0.7161 - val_loss: 11.8664 - val_mae: 1.7342 - lr: 6.5610e-04\n",
            "Epoch 47/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 1.3947 - mae: 0.7195 - val_loss: 10.8665 - val_mae: 1.6036 - lr: 6.5610e-04\n",
            "Epoch 48/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 1.0003 - mae: 0.6322 - val_loss: 9.4222 - val_mae: 1.3612 - lr: 6.5610e-04\n",
            "Epoch 49/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.8023 - mae: 0.5784 - val_loss: 8.7978 - val_mae: 1.3161 - lr: 6.5610e-04\n",
            "Epoch 50/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 2.1659 - mae: 0.8736 - val_loss: 18.9036 - val_mae: 2.6421 - lr: 6.5610e-04\n",
            "Epoch 51/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 2.5554 - mae: 0.9302 - val_loss: 19.2639 - val_mae: 2.6096 - lr: 6.5610e-04\n",
            "Epoch 52/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.2652 - mae: 0.6928 - val_loss: 11.6139 - val_mae: 1.7753 - lr: 6.5610e-04\n",
            "Epoch 53/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.8722 - mae: 0.5948 - val_loss: 9.2723 - val_mae: 1.5092 - lr: 6.5610e-04\n",
            "Epoch 54/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 1.8484 - mae: 0.7832\n",
            "Epoch 54: ReduceLROnPlateau reducing learning rate to 0.0005904900433961303.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.8538 - mae: 0.7838 - val_loss: 11.9398 - val_mae: 1.7026 - lr: 6.5610e-04\n",
            "Epoch 55/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 1.4316 - mae: 0.7395 - val_loss: 11.6780 - val_mae: 1.8385 - lr: 5.9049e-04\n",
            "Epoch 56/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.7697 - mae: 0.5751 - val_loss: 11.7257 - val_mae: 1.7581 - lr: 5.9049e-04\n",
            "Epoch 57/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 1.3897 - mae: 0.6963 - val_loss: 12.0316 - val_mae: 1.8114 - lr: 5.9049e-04\n",
            "Epoch 58/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 5.9003 - mae: 1.3517 - val_loss: 18.9280 - val_mae: 2.6728 - lr: 5.9049e-04\n",
            "Epoch 59/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 2.1104 - mae: 0.8480\n",
            "Epoch 59: ReduceLROnPlateau reducing learning rate to 0.0005314410547725857.\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 2.1289 - mae: 0.8491 - val_loss: 18.8019 - val_mae: 2.4137 - lr: 5.9049e-04\n",
            "Epoch 60/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 3.8040 - mae: 1.1183 - val_loss: 14.6563 - val_mae: 1.9576 - lr: 5.3144e-04\n",
            "Epoch 61/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.3313 - mae: 0.7189 - val_loss: 11.5548 - val_mae: 1.6375 - lr: 5.3144e-04\n",
            "Epoch 62/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.9103 - mae: 0.6216 - val_loss: 10.2101 - val_mae: 1.4353 - lr: 5.3144e-04\n",
            "Epoch 63/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.8048 - mae: 0.5973 - val_loss: 9.9855 - val_mae: 1.3851 - lr: 5.3144e-04\n",
            "Epoch 64/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.7473 - mae: 0.5800\n",
            "Epoch 64: ReduceLROnPlateau reducing learning rate to 0.00047829695977270604.\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.7548 - mae: 0.5807 - val_loss: 9.5896 - val_mae: 1.4329 - lr: 5.3144e-04\n",
            "Epoch 65/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.8122 - mae: 0.8153 - val_loss: 11.7859 - val_mae: 1.8402 - lr: 4.7830e-04\n",
            "Epoch 66/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.0427 - mae: 0.6570 - val_loss: 10.0578 - val_mae: 1.5793 - lr: 4.7830e-04\n",
            "Epoch 67/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.7358 - mae: 0.5736 - val_loss: 9.7113 - val_mae: 1.3853 - lr: 4.7830e-04\n",
            "Epoch 68/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.6653 - mae: 0.5507 - val_loss: 8.7461 - val_mae: 1.2363 - lr: 4.7830e-04\n",
            "Epoch 69/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.5943 - mae: 0.5274 - val_loss: 8.8629 - val_mae: 1.1780 - lr: 4.7830e-04\n",
            "Epoch 70/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.8384 - mae: 0.5822 - val_loss: 10.1910 - val_mae: 1.2851 - lr: 4.7830e-04\n",
            "Epoch 71/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.5938 - mae: 0.5174 - val_loss: 9.4481 - val_mae: 1.2308 - lr: 4.7830e-04\n",
            "Epoch 72/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.5413 - mae: 0.4937 - val_loss: 9.2065 - val_mae: 1.1738 - lr: 4.7830e-04\n",
            "Epoch 73/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.0416 - mae: 0.6513 - val_loss: 10.2822 - val_mae: 1.4921 - lr: 4.7830e-04\n",
            "Epoch 74/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.7989 - mae: 0.5777 - val_loss: 9.7202 - val_mae: 1.4279 - lr: 4.7830e-04\n",
            "Epoch 75/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.1977 - mae: 0.6653 - val_loss: 11.7157 - val_mae: 1.5518 - lr: 4.7830e-04\n",
            "Epoch 76/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.7169 - mae: 0.5445 - val_loss: 10.1387 - val_mae: 1.3103 - lr: 4.7830e-04\n",
            "Epoch 77/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.6197 - mae: 0.5257\n",
            "Epoch 77: ReduceLROnPlateau reducing learning rate to 0.0004304672533180565.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.6234 - mae: 0.5261 - val_loss: 9.6492 - val_mae: 1.3614 - lr: 4.7830e-04\n",
            "Epoch 78/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.0460 - mae: 0.6272 - val_loss: 9.1904 - val_mae: 1.4430 - lr: 4.3047e-04\n",
            "Epoch 79/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.5609 - mae: 0.5026 - val_loss: 8.0048 - val_mae: 1.1476 - lr: 4.3047e-04\n",
            "Epoch 80/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.5026 - mae: 0.4819 - val_loss: 8.2651 - val_mae: 1.1317 - lr: 4.3047e-04\n",
            "Epoch 81/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.5796 - mae: 0.5053 - val_loss: 8.2980 - val_mae: 1.1537 - lr: 4.3047e-04\n",
            "Epoch 82/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.7677 - mae: 0.5614 - val_loss: 9.7488 - val_mae: 1.2480 - lr: 4.3047e-04\n",
            "Epoch 83/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.5465 - mae: 0.5026 - val_loss: 9.3349 - val_mae: 1.2145 - lr: 4.3047e-04\n",
            "Epoch 84/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4720 - mae: 0.4734 - val_loss: 9.0041 - val_mae: 1.1339 - lr: 4.3047e-04\n",
            "Epoch 85/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.4739 - mae: 0.4669 - val_loss: 8.4672 - val_mae: 1.0852 - lr: 4.3047e-04\n",
            "Epoch 86/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.6768 - mae: 0.5286 - val_loss: 9.6320 - val_mae: 1.2209 - lr: 4.3047e-04\n",
            "Epoch 87/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.5505 - mae: 0.4884 - val_loss: 9.1077 - val_mae: 1.1448 - lr: 4.3047e-04\n",
            "Epoch 88/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.4918 - mae: 0.4667 - val_loss: 8.6784 - val_mae: 1.0587 - lr: 4.3047e-04\n",
            "Epoch 89/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4434 - mae: 0.4516 - val_loss: 9.7820 - val_mae: 1.1890 - lr: 4.3047e-04\n",
            "Epoch 90/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.4763 - mae: 0.4605 - val_loss: 9.5980 - val_mae: 1.1710 - lr: 4.3047e-04\n",
            "Epoch 91/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 1.1131 - mae: 0.5952 - val_loss: 14.2554 - val_mae: 1.9427 - lr: 4.3047e-04\n",
            "Epoch 92/300\n",
            "212/212 [==============================] - 17s 82ms/step - loss: 1.1271 - mae: 0.6141 - val_loss: 12.4034 - val_mae: 1.7954 - lr: 4.3047e-04\n",
            "Epoch 93/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.8287 - mae: 0.5691\n",
            "Epoch 93: ReduceLROnPlateau reducing learning rate to 0.00038742052274756136.\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.8288 - mae: 0.5691 - val_loss: 11.7755 - val_mae: 1.7750 - lr: 4.3047e-04\n",
            "Epoch 94/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.5260 - mae: 0.4776 - val_loss: 9.3038 - val_mae: 1.3625 - lr: 3.8742e-04\n",
            "Epoch 95/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.5512 - mae: 0.4747 - val_loss: 10.6557 - val_mae: 1.4990 - lr: 3.8742e-04\n",
            "Epoch 96/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.9164 - mae: 0.5532 - val_loss: 11.4792 - val_mae: 1.4678 - lr: 3.8742e-04\n",
            "Epoch 97/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.5967 - mae: 0.4816 - val_loss: 10.6297 - val_mae: 1.3710 - lr: 3.8742e-04\n",
            "Epoch 98/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.4787 - mae: 0.4570\n",
            "Epoch 98: ReduceLROnPlateau reducing learning rate to 0.0003486784757114947.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4840 - mae: 0.4576 - val_loss: 10.2138 - val_mae: 1.4716 - lr: 3.8742e-04\n",
            "Epoch 99/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.6227 - mae: 0.5161 - val_loss: 9.8649 - val_mae: 1.4206 - lr: 3.4868e-04\n",
            "Epoch 100/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.4528 - mae: 0.4581 - val_loss: 9.4221 - val_mae: 1.2244 - lr: 3.4868e-04\n",
            "Epoch 101/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.4188 - mae: 0.4451 - val_loss: 8.8814 - val_mae: 1.2557 - lr: 3.4868e-04\n",
            "Epoch 102/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.5273 - mae: 0.4820 - val_loss: 9.2608 - val_mae: 1.2315 - lr: 3.4868e-04\n",
            "Epoch 103/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.7152 - mae: 0.5206\n",
            "Epoch 103: ReduceLROnPlateau reducing learning rate to 0.00031381062290165574.\n",
            "212/212 [==============================] - 17s 82ms/step - loss: 0.7157 - mae: 0.5208 - val_loss: 10.0180 - val_mae: 1.4081 - lr: 3.4868e-04\n",
            "Epoch 104/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.5912 - mae: 0.4860 - val_loss: 9.9751 - val_mae: 1.2586 - lr: 3.1381e-04\n",
            "Epoch 105/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.3782 - mae: 0.4227 - val_loss: 9.1886 - val_mae: 1.0996 - lr: 3.1381e-04\n",
            "Epoch 106/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4941 - mae: 0.4498 - val_loss: 9.3034 - val_mae: 1.1887 - lr: 3.1381e-04\n",
            "Epoch 107/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.7337 - mae: 0.5071 - val_loss: 9.3615 - val_mae: 1.2065 - lr: 3.1381e-04\n",
            "Epoch 108/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.7424 - mae: 0.5286\n",
            "Epoch 108: ReduceLROnPlateau reducing learning rate to 0.0002824295632308349.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.7430 - mae: 0.5288 - val_loss: 9.8568 - val_mae: 1.2415 - lr: 3.1381e-04\n",
            "Epoch 109/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4312 - mae: 0.4454 - val_loss: 9.3155 - val_mae: 1.1098 - lr: 2.8243e-04\n",
            "Epoch 110/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4086 - mae: 0.4237 - val_loss: 9.0268 - val_mae: 1.1613 - lr: 2.8243e-04\n",
            "Epoch 111/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 1.0233 - mae: 0.5701 - val_loss: 10.9391 - val_mae: 1.3523 - lr: 2.8243e-04\n",
            "Epoch 112/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.4712 - mae: 0.4516 - val_loss: 10.6444 - val_mae: 1.2504 - lr: 2.8243e-04\n",
            "Epoch 113/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.3742 - mae: 0.4204\n",
            "Epoch 113: ReduceLROnPlateau reducing learning rate to 0.00025418660952709616.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3753 - mae: 0.4207 - val_loss: 9.6683 - val_mae: 1.1819 - lr: 2.8243e-04\n",
            "Epoch 114/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3829 - mae: 0.4249 - val_loss: 9.0805 - val_mae: 1.0683 - lr: 2.5419e-04\n",
            "Epoch 115/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3422 - mae: 0.4020 - val_loss: 9.4995 - val_mae: 1.1267 - lr: 2.5419e-04\n",
            "Epoch 116/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.7080 - mae: 0.5254 - val_loss: 11.5231 - val_mae: 1.4095 - lr: 2.5419e-04\n",
            "Epoch 117/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.3982 - mae: 0.4301 - val_loss: 10.2094 - val_mae: 1.1822 - lr: 2.5419e-04\n",
            "Epoch 118/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.3480 - mae: 0.4068\n",
            "Epoch 118: ReduceLROnPlateau reducing learning rate to 0.00022876793809700757.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3502 - mae: 0.4071 - val_loss: 9.2820 - val_mae: 1.1229 - lr: 2.5419e-04\n",
            "Epoch 119/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3613 - mae: 0.4130 - val_loss: 9.0165 - val_mae: 1.0634 - lr: 2.2877e-04\n",
            "Epoch 120/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3457 - mae: 0.4073 - val_loss: 8.9639 - val_mae: 1.0898 - lr: 2.2877e-04\n",
            "Epoch 121/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3225 - mae: 0.3946 - val_loss: 8.8272 - val_mae: 1.0656 - lr: 2.2877e-04\n",
            "Epoch 122/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.3183 - mae: 0.3905 - val_loss: 8.6148 - val_mae: 1.0580 - lr: 2.2877e-04\n",
            "Epoch 123/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.3035 - mae: 0.3855 - val_loss: 8.7448 - val_mae: 1.0128 - lr: 2.2877e-04\n",
            "Epoch 124/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3369 - mae: 0.3949 - val_loss: 8.4292 - val_mae: 1.0280 - lr: 2.2877e-04\n",
            "Epoch 125/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2942 - mae: 0.3764 - val_loss: 8.3404 - val_mae: 1.0139 - lr: 2.2877e-04\n",
            "Epoch 126/300\n",
            "212/212 [==============================] - 19s 90ms/step - loss: 0.3465 - mae: 0.3960 - val_loss: 8.5371 - val_mae: 1.0456 - lr: 2.2877e-04\n",
            "Epoch 127/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.3528 - mae: 0.4042 - val_loss: 8.5868 - val_mae: 1.0743 - lr: 2.2877e-04\n",
            "Epoch 128/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.5751 - mae: 0.4662\n",
            "Epoch 128: ReduceLROnPlateau reducing learning rate to 0.00020589114428730683.\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.5756 - mae: 0.4664 - val_loss: 9.1804 - val_mae: 1.1862 - lr: 2.2877e-04\n",
            "Epoch 129/300\n",
            "212/212 [==============================] - 19s 91ms/step - loss: 0.3610 - mae: 0.4057 - val_loss: 8.9335 - val_mae: 1.0768 - lr: 2.0589e-04\n",
            "Epoch 130/300\n",
            "212/212 [==============================] - 20s 94ms/step - loss: 0.3357 - mae: 0.4020 - val_loss: 9.0346 - val_mae: 1.1001 - lr: 2.0589e-04\n",
            "Epoch 131/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2935 - mae: 0.3773 - val_loss: 8.7080 - val_mae: 1.0846 - lr: 2.0589e-04\n",
            "Epoch 132/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2913 - mae: 0.3782 - val_loss: 8.5858 - val_mae: 1.1092 - lr: 2.0589e-04\n",
            "Epoch 133/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.3120 - mae: 0.3822\n",
            "Epoch 133: ReduceLROnPlateau reducing learning rate to 0.00018530203378759326.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3126 - mae: 0.3823 - val_loss: 8.7039 - val_mae: 1.0863 - lr: 2.0589e-04\n",
            "Epoch 134/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3047 - mae: 0.3815 - val_loss: 8.3801 - val_mae: 1.0636 - lr: 1.8530e-04\n",
            "Epoch 135/300\n",
            "212/212 [==============================] - 19s 90ms/step - loss: 0.3050 - mae: 0.3804 - val_loss: 8.3064 - val_mae: 0.9841 - lr: 1.8530e-04\n",
            "Epoch 136/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.3975 - mae: 0.4247 - val_loss: 9.0036 - val_mae: 1.1189 - lr: 1.8530e-04\n",
            "Epoch 137/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.5595 - mae: 0.4503 - val_loss: 9.8151 - val_mae: 1.2176 - lr: 1.8530e-04\n",
            "Epoch 138/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3448 - mae: 0.4074 - val_loss: 9.3121 - val_mae: 1.1764 - lr: 1.8530e-04\n",
            "Epoch 139/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.3323 - mae: 0.3903 - val_loss: 9.4143 - val_mae: 1.2994 - lr: 1.8530e-04\n",
            "Epoch 140/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.4122 - mae: 0.4374\n",
            "Epoch 140: ReduceLROnPlateau reducing learning rate to 0.00016677183302817866.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4191 - mae: 0.4381 - val_loss: 9.6812 - val_mae: 1.3194 - lr: 1.8530e-04\n",
            "Epoch 141/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4539 - mae: 0.4387 - val_loss: 9.1829 - val_mae: 1.2915 - lr: 1.6677e-04\n",
            "Epoch 142/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3251 - mae: 0.3938 - val_loss: 8.7277 - val_mae: 1.1318 - lr: 1.6677e-04\n",
            "Epoch 143/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3106 - mae: 0.3863 - val_loss: 8.3057 - val_mae: 1.0203 - lr: 1.6677e-04\n",
            "Epoch 144/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2921 - mae: 0.3785 - val_loss: 8.2934 - val_mae: 0.9724 - lr: 1.6677e-04\n",
            "Epoch 145/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2884 - mae: 0.3706 - val_loss: 8.1119 - val_mae: 0.9605 - lr: 1.6677e-04\n",
            "Epoch 146/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3812 - mae: 0.4136 - val_loss: 8.3071 - val_mae: 1.0092 - lr: 1.6677e-04\n",
            "Epoch 147/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.3137 - mae: 0.3856 - val_loss: 8.4010 - val_mae: 1.0085 - lr: 1.6677e-04\n",
            "Epoch 148/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3111 - mae: 0.3852 - val_loss: 8.8145 - val_mae: 1.0566 - lr: 1.6677e-04\n",
            "Epoch 149/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2937 - mae: 0.3767 - val_loss: 8.6203 - val_mae: 1.0713 - lr: 1.6677e-04\n",
            "Epoch 150/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.3453 - mae: 0.3887\n",
            "Epoch 150: ReduceLROnPlateau reducing learning rate to 0.00015009464841568844.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3473 - mae: 0.3891 - val_loss: 9.0635 - val_mae: 1.0626 - lr: 1.6677e-04\n",
            "Epoch 151/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2974 - mae: 0.3802 - val_loss: 8.6091 - val_mae: 1.0518 - lr: 1.5009e-04\n",
            "Epoch 152/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.3253 - mae: 0.3868 - val_loss: 8.2371 - val_mae: 0.9754 - lr: 1.5009e-04\n",
            "Epoch 153/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.4259 - mae: 0.4173 - val_loss: 8.7245 - val_mae: 1.0263 - lr: 1.5009e-04\n",
            "Epoch 154/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3008 - mae: 0.3792 - val_loss: 8.6410 - val_mae: 1.0240 - lr: 1.5009e-04\n",
            "Epoch 155/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.3111 - mae: 0.3829\n",
            "Epoch 155: ReduceLROnPlateau reducing learning rate to 0.0001350851875031367.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3110 - mae: 0.3829 - val_loss: 8.4303 - val_mae: 0.9951 - lr: 1.5009e-04\n",
            "Epoch 156/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2796 - mae: 0.3653 - val_loss: 8.4641 - val_mae: 0.9544 - lr: 1.3509e-04\n",
            "Epoch 157/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2774 - mae: 0.3661 - val_loss: 8.3943 - val_mae: 0.9467 - lr: 1.3509e-04\n",
            "Epoch 158/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2757 - mae: 0.3664 - val_loss: 8.4438 - val_mae: 0.9484 - lr: 1.3509e-04\n",
            "Epoch 159/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2663 - mae: 0.3593 - val_loss: 8.6026 - val_mae: 0.9522 - lr: 1.3509e-04\n",
            "Epoch 160/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2753 - mae: 0.3656 - val_loss: 8.6200 - val_mae: 0.9802 - lr: 1.3509e-04\n",
            "Epoch 161/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2614 - mae: 0.3581 - val_loss: 8.6019 - val_mae: 0.9708 - lr: 1.3509e-04\n",
            "Epoch 162/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2532 - mae: 0.3514\n",
            "Epoch 162: ReduceLROnPlateau reducing learning rate to 0.00012157666351413355.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2531 - mae: 0.3513 - val_loss: 8.4699 - val_mae: 0.9495 - lr: 1.3509e-04\n",
            "Epoch 163/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2459 - mae: 0.3439 - val_loss: 8.4591 - val_mae: 0.9373 - lr: 1.2158e-04\n",
            "Epoch 164/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2598 - mae: 0.3529 - val_loss: 8.6166 - val_mae: 0.9531 - lr: 1.2158e-04\n",
            "Epoch 165/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.2771 - mae: 0.3648 - val_loss: 8.4782 - val_mae: 0.9552 - lr: 1.2158e-04\n",
            "Epoch 166/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.3217 - mae: 0.3811 - val_loss: 8.4323 - val_mae: 0.9839 - lr: 1.2158e-04\n",
            "Epoch 167/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2857 - mae: 0.3667 - val_loss: 8.4227 - val_mae: 0.9685 - lr: 1.2158e-04\n",
            "Epoch 168/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2651 - mae: 0.3598\n",
            "Epoch 168: ReduceLROnPlateau reducing learning rate to 0.00010941899454337544.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2660 - mae: 0.3601 - val_loss: 8.5833 - val_mae: 0.9649 - lr: 1.2158e-04\n",
            "Epoch 169/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.2795 - mae: 0.3624 - val_loss: 8.6906 - val_mae: 0.9796 - lr: 1.0942e-04\n",
            "Epoch 170/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2607 - mae: 0.3548 - val_loss: 8.6693 - val_mae: 0.9901 - lr: 1.0942e-04\n",
            "Epoch 171/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2616 - mae: 0.3549 - val_loss: 8.8307 - val_mae: 0.9710 - lr: 1.0942e-04\n",
            "Epoch 172/300\n",
            "212/212 [==============================] - 17s 83ms/step - loss: 0.2631 - mae: 0.3558 - val_loss: 8.9253 - val_mae: 1.0022 - lr: 1.0942e-04\n",
            "Epoch 173/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2584 - mae: 0.3517\n",
            "Epoch 173: ReduceLROnPlateau reducing learning rate to 9.847709443420172e-05.\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2588 - mae: 0.3519 - val_loss: 9.0029 - val_mae: 0.9860 - lr: 1.0942e-04\n",
            "Epoch 174/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2628 - mae: 0.3581 - val_loss: 8.9671 - val_mae: 1.0243 - lr: 9.8477e-05\n",
            "Epoch 175/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2824 - mae: 0.3674 - val_loss: 9.1028 - val_mae: 1.0553 - lr: 9.8477e-05\n",
            "Epoch 176/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2655 - mae: 0.3547 - val_loss: 8.9439 - val_mae: 1.0227 - lr: 9.8477e-05\n",
            "Epoch 177/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2443 - mae: 0.3452 - val_loss: 8.6620 - val_mae: 0.9615 - lr: 9.8477e-05\n",
            "Epoch 178/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2560 - mae: 0.3509\n",
            "Epoch 178: ReduceLROnPlateau reducing learning rate to 8.862938630045391e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2560 - mae: 0.3509 - val_loss: 8.3357 - val_mae: 0.9532 - lr: 9.8477e-05\n",
            "Epoch 179/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2400 - mae: 0.3441 - val_loss: 8.3418 - val_mae: 0.9458 - lr: 8.8629e-05\n",
            "Epoch 180/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2380 - mae: 0.3446 - val_loss: 8.4015 - val_mae: 0.9430 - lr: 8.8629e-05\n",
            "Epoch 181/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2466 - mae: 0.3466 - val_loss: 8.4894 - val_mae: 0.9477 - lr: 8.8629e-05\n",
            "Epoch 182/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2531 - mae: 0.3477 - val_loss: 8.5344 - val_mae: 0.9475 - lr: 8.8629e-05\n",
            "Epoch 183/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2558 - mae: 0.3496\n",
            "Epoch 183: ReduceLROnPlateau reducing learning rate to 7.976644701557234e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2559 - mae: 0.3496 - val_loss: 8.8000 - val_mae: 0.9871 - lr: 8.8629e-05\n",
            "Epoch 184/300\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.2361 - mae: 0.3394 - val_loss: 8.4402 - val_mae: 0.9711 - lr: 7.9766e-05\n",
            "Epoch 185/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.2383 - mae: 0.3377 - val_loss: 8.4274 - val_mae: 0.9411 - lr: 7.9766e-05\n",
            "Epoch 186/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2477 - mae: 0.3455 - val_loss: 8.6440 - val_mae: 0.9541 - lr: 7.9766e-05\n",
            "Epoch 187/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2407 - mae: 0.3424 - val_loss: 8.7131 - val_mae: 0.9594 - lr: 7.9766e-05\n",
            "Epoch 188/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2325 - mae: 0.3350\n",
            "Epoch 188: ReduceLROnPlateau reducing learning rate to 7.178980231401511e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2325 - mae: 0.3350 - val_loss: 8.7073 - val_mae: 0.9510 - lr: 7.9766e-05\n",
            "Epoch 189/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2395 - mae: 0.3417 - val_loss: 8.6489 - val_mae: 0.9549 - lr: 7.1790e-05\n",
            "Epoch 190/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2321 - mae: 0.3359 - val_loss: 8.6335 - val_mae: 0.9368 - lr: 7.1790e-05\n",
            "Epoch 191/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2302 - mae: 0.3361 - val_loss: 8.3751 - val_mae: 0.9238 - lr: 7.1790e-05\n",
            "Epoch 192/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2329 - mae: 0.3346 - val_loss: 8.4704 - val_mae: 0.9154 - lr: 7.1790e-05\n",
            "Epoch 193/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2364 - mae: 0.3364 - val_loss: 8.3878 - val_mae: 0.9138 - lr: 7.1790e-05\n",
            "Epoch 194/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2410 - mae: 0.3434 - val_loss: 8.6105 - val_mae: 0.9659 - lr: 7.1790e-05\n",
            "Epoch 195/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2342 - mae: 0.3371 - val_loss: 8.5871 - val_mae: 0.9432 - lr: 7.1790e-05\n",
            "Epoch 196/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2508 - mae: 0.3319 - val_loss: 8.4875 - val_mae: 0.9668 - lr: 7.1790e-05\n",
            "Epoch 197/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2734 - mae: 0.3539 - val_loss: 8.6116 - val_mae: 1.0005 - lr: 7.1790e-05\n",
            "Epoch 198/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2532 - mae: 0.3466\n",
            "Epoch 198: ReduceLROnPlateau reducing learning rate to 6.461082011810504e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2532 - mae: 0.3466 - val_loss: 9.0199 - val_mae: 1.0366 - lr: 7.1790e-05\n",
            "Epoch 199/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2393 - mae: 0.3395 - val_loss: 8.8148 - val_mae: 0.9995 - lr: 6.4611e-05\n",
            "Epoch 200/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2300 - mae: 0.3321 - val_loss: 8.7857 - val_mae: 0.9796 - lr: 6.4611e-05\n",
            "Epoch 201/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2386 - mae: 0.3388 - val_loss: 8.7448 - val_mae: 0.9947 - lr: 6.4611e-05\n",
            "Epoch 202/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2303 - mae: 0.3329 - val_loss: 8.6730 - val_mae: 0.9733 - lr: 6.4611e-05\n",
            "Epoch 203/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2365 - mae: 0.3383\n",
            "Epoch 203: ReduceLROnPlateau reducing learning rate to 5.8149741380475466e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2391 - mae: 0.3387 - val_loss: 8.6830 - val_mae: 0.9751 - lr: 6.4611e-05\n",
            "Epoch 204/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2306 - mae: 0.3343 - val_loss: 8.6293 - val_mae: 0.9462 - lr: 5.8150e-05\n",
            "Epoch 205/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.2199 - mae: 0.3270 - val_loss: 8.5969 - val_mae: 0.9376 - lr: 5.8150e-05\n",
            "Epoch 206/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2195 - mae: 0.3288 - val_loss: 8.5734 - val_mae: 0.9369 - lr: 5.8150e-05\n",
            "Epoch 207/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2189 - mae: 0.3272 - val_loss: 8.5432 - val_mae: 0.9408 - lr: 5.8150e-05\n",
            "Epoch 208/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2288 - mae: 0.3311\n",
            "Epoch 208: ReduceLROnPlateau reducing learning rate to 5.233476658759173e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2288 - mae: 0.3311 - val_loss: 8.6120 - val_mae: 0.9372 - lr: 5.8150e-05\n",
            "Epoch 209/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2275 - mae: 0.3276 - val_loss: 8.5409 - val_mae: 0.9349 - lr: 5.2335e-05\n",
            "Epoch 210/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2337 - mae: 0.3360 - val_loss: 8.5011 - val_mae: 0.9567 - lr: 5.2335e-05\n",
            "Epoch 211/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2200 - mae: 0.3254 - val_loss: 8.5026 - val_mae: 0.9728 - lr: 5.2335e-05\n",
            "Epoch 212/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2375 - mae: 0.3377 - val_loss: 8.5390 - val_mae: 1.0031 - lr: 5.2335e-05\n",
            "Epoch 213/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2364 - mae: 0.3346\n",
            "Epoch 213: ReduceLROnPlateau reducing learning rate to 4.7101289601414466e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2379 - mae: 0.3349 - val_loss: 8.8103 - val_mae: 1.0026 - lr: 5.2335e-05\n",
            "Epoch 214/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2341 - mae: 0.3326 - val_loss: 8.6586 - val_mae: 0.9775 - lr: 4.7101e-05\n",
            "Epoch 215/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2256 - mae: 0.3321 - val_loss: 8.6366 - val_mae: 0.9505 - lr: 4.7101e-05\n",
            "Epoch 216/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2259 - mae: 0.3292 - val_loss: 8.7028 - val_mae: 0.9470 - lr: 4.7101e-05\n",
            "Epoch 217/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2344 - mae: 0.3358 - val_loss: 9.5642 - val_mae: 1.0224 - lr: 4.7101e-05\n",
            "Epoch 218/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2330 - mae: 0.3351\n",
            "Epoch 218: ReduceLROnPlateau reducing learning rate to 4.239116096869111e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2329 - mae: 0.3351 - val_loss: 9.2677 - val_mae: 0.9944 - lr: 4.7101e-05\n",
            "Epoch 219/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2237 - mae: 0.3308 - val_loss: 9.1553 - val_mae: 0.9807 - lr: 4.2391e-05\n",
            "Epoch 220/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2218 - mae: 0.3288 - val_loss: 9.0277 - val_mae: 0.9750 - lr: 4.2391e-05\n",
            "Epoch 221/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.2241 - mae: 0.3298 - val_loss: 8.9980 - val_mae: 0.9613 - lr: 4.2391e-05\n",
            "Epoch 222/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2207 - mae: 0.3246 - val_loss: 9.0360 - val_mae: 0.9690 - lr: 4.2391e-05\n",
            "Epoch 223/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2218 - mae: 0.3270\n",
            "Epoch 223: ReduceLROnPlateau reducing learning rate to 3.815204618149437e-05.\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2223 - mae: 0.3271 - val_loss: 9.0187 - val_mae: 0.9950 - lr: 4.2391e-05\n",
            "Epoch 224/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2207 - mae: 0.3263 - val_loss: 8.9847 - val_mae: 0.9756 - lr: 3.8152e-05\n",
            "Epoch 225/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2214 - mae: 0.3278 - val_loss: 8.7930 - val_mae: 0.9552 - lr: 3.8152e-05\n",
            "Epoch 226/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2179 - mae: 0.3227 - val_loss: 8.7834 - val_mae: 0.9474 - lr: 3.8152e-05\n",
            "Epoch 227/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2163 - mae: 0.3238 - val_loss: 8.7307 - val_mae: 0.9398 - lr: 3.8152e-05\n",
            "Epoch 228/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2152 - mae: 0.3223\n",
            "Epoch 228: ReduceLROnPlateau reducing learning rate to 3.4336842873017304e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2161 - mae: 0.3226 - val_loss: 8.7489 - val_mae: 0.9385 - lr: 3.8152e-05\n",
            "Epoch 229/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2236 - mae: 0.3275 - val_loss: 8.8126 - val_mae: 0.9485 - lr: 3.4337e-05\n",
            "Epoch 230/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2186 - mae: 0.3247 - val_loss: 8.7825 - val_mae: 0.9398 - lr: 3.4337e-05\n",
            "Epoch 231/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2215 - mae: 0.3256 - val_loss: 8.8567 - val_mae: 0.9487 - lr: 3.4337e-05\n",
            "Epoch 232/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2177 - mae: 0.3247 - val_loss: 9.0057 - val_mae: 0.9558 - lr: 3.4337e-05\n",
            "Epoch 233/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2147 - mae: 0.3218\n",
            "Epoch 233: ReduceLROnPlateau reducing learning rate to 3.0903160222806036e-05.\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2147 - mae: 0.3218 - val_loss: 8.9256 - val_mae: 0.9455 - lr: 3.4337e-05\n",
            "Epoch 234/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2117 - mae: 0.3197 - val_loss: 8.9055 - val_mae: 0.9474 - lr: 3.0903e-05\n",
            "Epoch 235/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2110 - mae: 0.3205 - val_loss: 8.8409 - val_mae: 0.9452 - lr: 3.0903e-05\n",
            "Epoch 236/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2141 - mae: 0.3202 - val_loss: 8.8250 - val_mae: 0.9459 - lr: 3.0903e-05\n",
            "Epoch 237/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2200 - mae: 0.3233 - val_loss: 8.7888 - val_mae: 0.9477 - lr: 3.0903e-05\n",
            "Epoch 238/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2132 - mae: 0.3195\n",
            "Epoch 238: ReduceLROnPlateau reducing learning rate to 2.7812844200525434e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2132 - mae: 0.3195 - val_loss: 8.7458 - val_mae: 0.9594 - lr: 3.0903e-05\n",
            "Epoch 239/300\n",
            "212/212 [==============================] - 18s 84ms/step - loss: 0.2140 - mae: 0.3215 - val_loss: 8.7559 - val_mae: 0.9408 - lr: 2.7813e-05\n",
            "Epoch 240/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2197 - mae: 0.3239 - val_loss: 8.7709 - val_mae: 0.9386 - lr: 2.7813e-05\n",
            "Epoch 241/300\n",
            "212/212 [==============================] - 18s 87ms/step - loss: 0.2128 - mae: 0.3208 - val_loss: 8.6546 - val_mae: 0.9325 - lr: 2.7813e-05\n",
            "Epoch 242/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2129 - mae: 0.3191 - val_loss: 8.6303 - val_mae: 0.9331 - lr: 2.7813e-05\n",
            "Epoch 243/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2164 - mae: 0.3239\n",
            "Epoch 243: ReduceLROnPlateau reducing learning rate to 2.5031560107890984e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2167 - mae: 0.3241 - val_loss: 8.5824 - val_mae: 0.9283 - lr: 2.7813e-05\n",
            "Epoch 244/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2143 - mae: 0.3196 - val_loss: 8.4843 - val_mae: 0.9312 - lr: 2.5032e-05\n",
            "Epoch 245/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2170 - mae: 0.3204 - val_loss: 8.4772 - val_mae: 0.9284 - lr: 2.5032e-05\n",
            "Epoch 246/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2144 - mae: 0.3208 - val_loss: 8.5664 - val_mae: 0.9325 - lr: 2.5032e-05\n",
            "Epoch 247/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2120 - mae: 0.3171 - val_loss: 8.4695 - val_mae: 0.9284 - lr: 2.5032e-05\n",
            "Epoch 248/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2148 - mae: 0.3198\n",
            "Epoch 248: ReduceLROnPlateau reducing learning rate to 2.2528404588229024e-05.\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2165 - mae: 0.3202 - val_loss: 8.5186 - val_mae: 0.9299 - lr: 2.5032e-05\n",
            "Epoch 249/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2171 - mae: 0.3223 - val_loss: 8.4838 - val_mae: 0.9303 - lr: 2.2528e-05\n",
            "Epoch 250/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2146 - mae: 0.3208 - val_loss: 8.4867 - val_mae: 0.9272 - lr: 2.2528e-05\n",
            "Epoch 251/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2062 - mae: 0.3160 - val_loss: 8.4925 - val_mae: 0.9349 - lr: 2.2528e-05\n",
            "Epoch 252/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2164 - mae: 0.3224 - val_loss: 8.5567 - val_mae: 0.9471 - lr: 2.2528e-05\n",
            "Epoch 253/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2180 - mae: 0.3218\n",
            "Epoch 253: ReduceLROnPlateau reducing learning rate to 2.0275563474569936e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2181 - mae: 0.3219 - val_loss: 8.4320 - val_mae: 0.9441 - lr: 2.2528e-05\n",
            "Epoch 254/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2164 - mae: 0.3223 - val_loss: 8.5025 - val_mae: 0.9373 - lr: 2.0276e-05\n",
            "Epoch 255/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2131 - mae: 0.3199 - val_loss: 8.4809 - val_mae: 0.9303 - lr: 2.0276e-05\n",
            "Epoch 256/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2290 - mae: 0.3224 - val_loss: 8.4523 - val_mae: 0.9361 - lr: 2.0276e-05\n",
            "Epoch 257/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2186 - mae: 0.3242 - val_loss: 8.8012 - val_mae: 0.9771 - lr: 2.0276e-05\n",
            "Epoch 258/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2198 - mae: 0.3236\n",
            "Epoch 258: ReduceLROnPlateau reducing learning rate to 1.8248007290821987e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2203 - mae: 0.3238 - val_loss: 8.6906 - val_mae: 0.9596 - lr: 2.0276e-05\n",
            "Epoch 259/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2107 - mae: 0.3187 - val_loss: 8.5901 - val_mae: 0.9413 - lr: 1.8248e-05\n",
            "Epoch 260/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2163 - mae: 0.3233 - val_loss: 8.5928 - val_mae: 0.9406 - lr: 1.8248e-05\n",
            "Epoch 261/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2137 - mae: 0.3206 - val_loss: 8.5943 - val_mae: 0.9416 - lr: 1.8248e-05\n",
            "Epoch 262/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2087 - mae: 0.3171 - val_loss: 8.5957 - val_mae: 0.9427 - lr: 1.8248e-05\n",
            "Epoch 263/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2069 - mae: 0.3151\n",
            "Epoch 263: ReduceLROnPlateau reducing learning rate to 1.6423206398030745e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2081 - mae: 0.3153 - val_loss: 8.5926 - val_mae: 0.9385 - lr: 1.8248e-05\n",
            "Epoch 264/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2131 - mae: 0.3203 - val_loss: 8.6088 - val_mae: 0.9372 - lr: 1.6423e-05\n",
            "Epoch 265/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2127 - mae: 0.3181 - val_loss: 8.6531 - val_mae: 0.9476 - lr: 1.6423e-05\n",
            "Epoch 266/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2111 - mae: 0.3175 - val_loss: 8.6678 - val_mae: 0.9444 - lr: 1.6423e-05\n",
            "Epoch 267/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2152 - mae: 0.3189 - val_loss: 8.6616 - val_mae: 0.9450 - lr: 1.6423e-05\n",
            "Epoch 268/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2152 - mae: 0.3208\n",
            "Epoch 268: ReduceLROnPlateau reducing learning rate to 1.4780885430809576e-05.\n",
            "212/212 [==============================] - 18s 83ms/step - loss: 0.2162 - mae: 0.3210 - val_loss: 8.6457 - val_mae: 0.9390 - lr: 1.6423e-05\n",
            "Epoch 269/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2128 - mae: 0.3188 - val_loss: 8.6922 - val_mae: 0.9525 - lr: 1.4781e-05\n",
            "Epoch 270/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2097 - mae: 0.3169 - val_loss: 8.6354 - val_mae: 0.9452 - lr: 1.4781e-05\n",
            "Epoch 271/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2054 - mae: 0.3127 - val_loss: 8.5320 - val_mae: 0.9376 - lr: 1.4781e-05\n",
            "Epoch 272/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2076 - mae: 0.3156 - val_loss: 8.5421 - val_mae: 0.9371 - lr: 1.4781e-05\n",
            "Epoch 273/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2099 - mae: 0.3149\n",
            "Epoch 273: ReduceLROnPlateau reducing learning rate to 1.3302796560310526e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2101 - mae: 0.3150 - val_loss: 8.5203 - val_mae: 0.9373 - lr: 1.4781e-05\n",
            "Epoch 274/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2081 - mae: 0.3152 - val_loss: 8.5405 - val_mae: 0.9336 - lr: 1.3303e-05\n",
            "Epoch 275/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2071 - mae: 0.3155 - val_loss: 8.5186 - val_mae: 0.9425 - lr: 1.3303e-05\n",
            "Epoch 276/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2051 - mae: 0.3129 - val_loss: 8.5282 - val_mae: 0.9373 - lr: 1.3303e-05\n",
            "Epoch 277/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2124 - mae: 0.3191 - val_loss: 8.5533 - val_mae: 0.9431 - lr: 1.3303e-05\n",
            "Epoch 278/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2037 - mae: 0.3112\n",
            "Epoch 278: ReduceLROnPlateau reducing learning rate to 1.1972517313552089e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2042 - mae: 0.3113 - val_loss: 8.6003 - val_mae: 0.9461 - lr: 1.3303e-05\n",
            "Epoch 279/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2127 - mae: 0.3195 - val_loss: 8.5788 - val_mae: 0.9430 - lr: 1.1973e-05\n",
            "Epoch 280/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2117 - mae: 0.3186 - val_loss: 8.5692 - val_mae: 0.9355 - lr: 1.1973e-05\n",
            "Epoch 281/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2041 - mae: 0.3121 - val_loss: 8.5540 - val_mae: 0.9373 - lr: 1.1973e-05\n",
            "Epoch 282/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2089 - mae: 0.3156 - val_loss: 8.5619 - val_mae: 0.9428 - lr: 1.1973e-05\n",
            "Epoch 283/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2074 - mae: 0.3150\n",
            "Epoch 283: ReduceLROnPlateau reducing learning rate to 1.077526558219688e-05.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2083 - mae: 0.3153 - val_loss: 8.6409 - val_mae: 0.9454 - lr: 1.1973e-05\n",
            "Epoch 284/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2068 - mae: 0.3152 - val_loss: 8.6902 - val_mae: 0.9601 - lr: 1.0775e-05\n",
            "Epoch 285/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2078 - mae: 0.3138 - val_loss: 8.7166 - val_mae: 0.9560 - lr: 1.0775e-05\n",
            "Epoch 286/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2058 - mae: 0.3124 - val_loss: 8.7000 - val_mae: 0.9572 - lr: 1.0775e-05\n",
            "Epoch 287/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2044 - mae: 0.3118 - val_loss: 8.6803 - val_mae: 0.9492 - lr: 1.0775e-05\n",
            "Epoch 288/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2054 - mae: 0.3151\n",
            "Epoch 288: ReduceLROnPlateau reducing learning rate to 9.697739187686238e-06.\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2065 - mae: 0.3154 - val_loss: 8.6662 - val_mae: 0.9503 - lr: 1.0775e-05\n",
            "Epoch 289/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2077 - mae: 0.3136 - val_loss: 8.6917 - val_mae: 0.9545 - lr: 9.6977e-06\n",
            "Epoch 290/300\n",
            "212/212 [==============================] - 19s 88ms/step - loss: 0.2090 - mae: 0.3148 - val_loss: 8.6703 - val_mae: 0.9458 - lr: 9.6977e-06\n",
            "Epoch 291/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2058 - mae: 0.3145 - val_loss: 8.6809 - val_mae: 0.9431 - lr: 9.6977e-06\n",
            "Epoch 292/300\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2092 - mae: 0.3147 - val_loss: 8.6624 - val_mae: 0.9415 - lr: 9.6977e-06\n",
            "Epoch 293/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2072 - mae: 0.3150\n",
            "Epoch 293: ReduceLROnPlateau reducing learning rate to 8.727965268917615e-06.\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2072 - mae: 0.3150 - val_loss: 8.6158 - val_mae: 0.9367 - lr: 9.6977e-06\n",
            "Epoch 294/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2083 - mae: 0.3142 - val_loss: 8.5939 - val_mae: 0.9368 - lr: 8.7280e-06\n",
            "Epoch 295/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2042 - mae: 0.3124 - val_loss: 8.6252 - val_mae: 0.9449 - lr: 8.7280e-06\n",
            "Epoch 296/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2042 - mae: 0.3127 - val_loss: 8.6718 - val_mae: 0.9484 - lr: 8.7280e-06\n",
            "Epoch 297/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2079 - mae: 0.3157 - val_loss: 8.6635 - val_mae: 0.9473 - lr: 8.7280e-06\n",
            "Epoch 298/300\n",
            "211/212 [============================>.] - ETA: 0s - loss: 0.2027 - mae: 0.3114\n",
            "Epoch 298: ReduceLROnPlateau reducing learning rate to 7.855168496462283e-06.\n",
            "212/212 [==============================] - 18s 86ms/step - loss: 0.2030 - mae: 0.3116 - val_loss: 8.6636 - val_mae: 0.9435 - lr: 8.7280e-06\n",
            "Epoch 299/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2021 - mae: 0.3115 - val_loss: 8.6915 - val_mae: 0.9439 - lr: 7.8552e-06\n",
            "Epoch 300/300\n",
            "212/212 [==============================] - 18s 85ms/step - loss: 0.2044 - mae: 0.3136 - val_loss: 8.6330 - val_mae: 0.9381 - lr: 7.8552e-06\n"
          ]
        }
      ],
      "source": [
        "EPOCHS=300\n",
        "BATCH_SIZE=64\n",
        "\n",
        "history=model.fit(X_train,\n",
        "                 Y_train,\n",
        "                 validation_data=(X_test,Y_test),\n",
        "                 batch_size=BATCH_SIZE,\n",
        "                 epochs=EPOCHS,\n",
        "                 callbacks=[model_checkpoint,reduce_lr],\n",
        "                  steps_per_epoch=212\n",
        "                  \n",
        "\n",
        "                 )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "yJJZWaRaND22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5443f830-9259-4a6d-f191-61227ba7ab83"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[25.06678   ],\n",
              "       [28.896982  ],\n",
              "       [ 4.7628436 ],\n",
              "       [14.1008005 ],\n",
              "       [20.367718  ],\n",
              "       [27.802975  ],\n",
              "       [ 6.89298   ],\n",
              "       [22.013563  ],\n",
              "       [28.91738   ],\n",
              "       [ 9.054654  ],\n",
              "       [20.126543  ],\n",
              "       [16.924988  ],\n",
              "       [18.190886  ],\n",
              "       [ 8.913317  ],\n",
              "       [15.081546  ],\n",
              "       [ 6.7801213 ],\n",
              "       [13.7944145 ],\n",
              "       [ 1.0407615 ],\n",
              "       [-0.07920676],\n",
              "       [22.609425  ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "prediction_val=model.predict(X_test,batch_size=64)\n",
        "prediction_val[:20]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(ckp_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGmunyexSx1y",
        "outputId": "bd57d845-84e4-4ae1-be68-184646742919"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f5bd0d83e90>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "OGRvXHTIND57",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "227381a3-c903-4ed3-c3b9-1b5b9d5c5e90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_96_input with unsupported characters which will be renamed to mobilenetv2_1_00_96_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp91qx14zp/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp91qx14zp/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model_new.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "iIhOP2RoND80"
      },
      "outputs": [],
      "source": [
        "TF_LITE_MODEL_FILE_NAME = 'model_new.tflite'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "interpreter = tf.lite.Interpreter(model_path = TF_LITE_MODEL_FILE_NAME)\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "print(\"Input Shape:\", input_details[0]['shape'])\n",
        "print(\"Input Type:\", input_details[0]['dtype'])\n",
        "print(\"Output Shape:\", output_details[0]['shape'])\n",
        "print(\"Output Type:\", output_details[0]['dtype'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLAZ9WUfV__P",
        "outputId": "adeeb48b-3da7-47a3-c50f-9789ba9bf883"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input Shape: [ 1 96 96  3]\n",
            "Input Type: <class 'numpy.float32'>\n",
            "Output Shape: [1 1]\n",
            "Output Type: <class 'numpy.float32'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPg0EaZHWAB0",
        "outputId": "4f1a9a5d-8026-44e8-930d-ee843cccad81"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2384, 96, 96, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_test = np.zeros( (2384, 96, 96, 3) )\n",
        "X_test = X_test[:3, :, ]\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iozguz6TZ7MF",
        "outputId": "919dacd3-84e9-4161-cdab-a631426c3898"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 96, 96, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_image = np.expand_dims(X_test, axis=0)\n",
        "test_image=np.array(X_test,dtype=np.float32)\n"
      ],
      "metadata": {
        "id": "s6t75eN4WAEK"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(interpreter.get_input_details())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ep1Bsv8ZEzw",
        "outputId": "c9573ea8-d138-4c37-ebab-2ecb6d9abf06"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'serving_default_mobilenetv2_1.00_96_input:0', 'index': 0, 'shape': array([ 1, 96, 96,  3], dtype=int32), 'shape_signature': array([-1, 96, 96,  3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "filename='pred_model'\n",
        "pickle.dump(model,open(filename,'wb'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKywwBeZ8us5",
        "outputId": "f205c3a3-a939-4efa-a95f-2d2348b95769"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_96_input with unsupported characters which will be renamed to mobilenetv2_1_00_96_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://dc23d7db-856c-44af-a905-566212610d24/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://dc23d7db-856c-44af-a905-566212610d24/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model=pickle.load(open(filename,'rb'))\n",
        "loaded_model.predict(X_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YdYmNlBA8uqA",
        "outputId": "9fea4edb-b8c3-4136-9a88-3aebbc6f2b1e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9.465995],\n",
              "       [9.465995],\n",
              "       [9.465995]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('pickle_model','wb') as f:\n",
        "  pickle.dump(model,f )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZZPBD4nKMeZ",
        "outputId": "cd0d44f8-0ef4-45bf-b929-7919459983fa"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_96_input with unsupported characters which will be renamed to mobilenetv2_1_00_96_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: ram://9bf5d5ed-3d5a-456d-a582-36b8fd67ce0b/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: ram://9bf5d5ed-3d5a-456d-a582-36b8fd67ce0b/assets\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model.\n",
        "with open('model_new_facerec.tflite', 'wb') as f:\n",
        "  f.write(tflite_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gunfxI-bKrSo",
        "outputId": "f50da48a-6fb3-4918-9d9b-a7f74a0ee9ed"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_96_input with unsupported characters which will be renamed to mobilenetv2_1_00_96_input in the SavedModel.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp0g158y80/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /tmp/tmp0g158y80/assets\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "img = image.load_img('/content/test.jpeg',target_size=(96,96))\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "\n",
        "x=np.expand_dims(x,axis=0)\n",
        "\n",
        "x=preprocess_input(x)"
      ],
      "metadata": {
        "id": "7OWaDbhSLhqV"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred = model.predict(x)\n",
        "pred\n",
        "num=round(pred[0][0])\n",
        "print(index_2_name[num])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIrvbNcNLhyH",
        "outputId": "682a4fae-ada9-42e3-9989-0d225a969279"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "David_Schwimmer\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "face_recognition_test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMXRCzkJ30EiLb85nW1na80",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}